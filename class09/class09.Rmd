---
title: "class09"
author: "Jaya George"
date: "4/30/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
fna.data <- "WisconsinCancer.csv"
```

```{r}
wisc.df <- read.csv(fna.data)
wisc.df
```

#Exploratory data Analysis

Convert the features of the data: wisc.data
```{r}
#Omits the last column 33 which is an error
wisc.data <- as.matrix(wisc.df[3:32])
head(wisc.data)
```

Set the row names of wisc.data
```{r}
row.names(wisc.data) <- wisc.df$id
```

Create diagnosis vector by completing the missing code
```{r}
#set the diagnosis column to M and if its true it will input a 1
diagnosis <- as.numeric(wisc.df$diagnosis== "M")
diagnosis
```

Q1. How many patients are in the dataset?
```{r}
nrow(wisc.df)
```

Q2. How many variables/features in the data are suffixed with _mean?
```{r}
#grep tells you where the pattern is within your data set and length counts how many there are
length(grep("_mean", colnames(wisc.data)))
```

Q3. How many of the observations have malignant diagnoses?
```{r}
mal <- table(wisc.df$diagnosis)
mal
```


#PCA

Check column means and standard deviations
```{r}
round( colMeans(wisc.data), 1)
```

```{r}
#run standard deviation funciton along columns which correlates to the 2
round( apply(wisc.data,2,sd), 1)
```

```{r}
#from apply and colMeans can gather that there is large variation in data and needs to scale
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

Following questions are found in summary(wisc.pr)
Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
44.27%

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
three PC

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
seven PC

```{r}
biplot(wisc.pr)
```


Q7.  What stands out to you about this plot? Is it easy or difficult to understand? Why?
difficult to understand becuase our data set is so large

Scatter plot observations by components 1 and 2
```{r}
#black dots are benign and red are malignant based on diagnosis vector 
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis+1, xlab="PC1", ylab="PC2")
```

Repeat for components 1 and 3
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis+1, xlab="PC1", ylab="PC2")
```

Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner cut separating the two subgroups.
Overall, the plots indicate that principal component 1 is capturing a separation of malignant from benign samples. This is an important and interesting result worthy of further exploration - as we will do in the next sections!

Calculate variance of each component:
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Variance explained by each principal component: pve
```{r}
pve <- (pr.var / sum(pr.var)) *100
pve
```

Plot variance explained for each principal component
```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")
```

Alternative scree plot of the same data, note data driven y-axis
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Plot cumulative proportion of variance explained
```{r}
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")
```

```{r}
par(mfrow=c(1,2))
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")
```

```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

Q9. For the first principal component, what is the component of the loading vector (i.e.  wisc.pr$rotation[,1]) for the feature concave.points_mean?
 concave.points_mean= -0.26085376
```{r}
wisc.pr$rotation[,1]
```


Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
five

#Hierarchical clustering

Scale the wisc.data data: data.scaled
```{r}
data.scaled <- scale(wisc.data)
```

distances between all pairs of observations in the new scaled dataset
```{r}
data.dist <- dist(data.scaled)
```

Create a hierarchical clustering model using complete linkage
```{r}
wisc.hclust <- hclust(data.dist, method="complete")
```

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
h=19
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, 4, 19)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?
```{r}
wisc.hclust.clusters2 <- cutree(wisc.hclust, 2, 30)
table(wisc.hclust.clusters2, diagnosis)
```

```{r}
wisc.hclust.clusters10 <- cutree(wisc.hclust, 10, 20)
table(wisc.hclust.clusters10, diagnosis)
```

#Clustering on PCA results
Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Ward’s criterion here because it is based on multidimensional variance like principal components analysis. 

```{r}
wisc.pr.hclust <- hclust( dist( wisc.pr$x[,1:7]), method="ward.D2")
```

```{r}
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis+1)
```

#Sensitivity/ Specificity

Q16. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

*Sensitivity* (false positives) refers to a test’s ability to correctly detect ill patients who do have the condition. In our example here the sensitivity is the total number of samples in the cluster identified as predominantly malignant (cancerous) divided by the total number of known malignant samples.

*Specificity* (false negatives) relates to a test’s ability to correctly reject healthy patients without a condition. In our example specificity is the proportion of benign (not cancerous) samples in the cluster identified as predominantly benign that are known to be benign.


#Prediction

```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16)
```

